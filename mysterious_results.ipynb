{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31594060.0\n",
      "500 1.3092007066006772e-05\n",
      "1000 1.9149656509398483e-06\n",
      "1500 8.756911711316206e-07\n",
      "2000 5.159846523383749e-07\n",
      "2500 3.619117308062414e-07\n",
      "3000 2.992068459661823e-07\n",
      "3500 2.3256262693394092e-07\n",
      "4000 2.0740695561016764e-07\n",
      "4500 1.7284612852108694e-07\n",
      "5000 1.540995810955792e-07\n",
      "5500 1.4288161764852703e-07\n",
      "6000 1.2391731729621824e-07\n",
      "6500 1.120927421993656e-07\n",
      "7000 1.132237557044391e-07\n",
      "7500 9.936493938766944e-08\n",
      "8000 8.687638342053106e-08\n",
      "8500 8.070866641674002e-08\n",
      "9000 8.007982899016497e-08\n",
      "9500 8.192574796339613e-08\n",
      "10000 7.20720692015675e-08\n",
      "10500 6.80708325262458e-08\n",
      "11000 7.241099808652507e-08\n",
      "11500 6.348947323431275e-08\n",
      "12000 6.48314681939155e-08\n",
      "12500 6.171907784846553e-08\n",
      "13000 6.255173445879336e-08\n",
      "13500 6.563429622019612e-08\n",
      "14000 5.877193132164393e-08\n",
      "14500 5.917036816072141e-08\n",
      "15000 6.654447304299538e-08\n",
      "10.645319 seconds\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_custom_function.py\n",
    "import torch\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "import time\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  We can implement our own custom autograd Functions by subclassing\n",
    "  torch.autograd.Function and implementing the forward and backward passes\n",
    "  which operate on Tensors.\n",
    "  \"\"\"\n",
    "  @staticmethod\n",
    "  def forward(ctx, x):\n",
    "    \"\"\"\n",
    "    In the forward pass we receive a context object and a Tensor containing the\n",
    "    input; we must return a Tensor containing the output, and we can use the\n",
    "    context object to cache objects for use in the backward pass.\n",
    "    \"\"\"\n",
    "    ctx.save_for_backward(x)\n",
    "    return x.clamp(min=0)\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive the context object and a Tensor containing\n",
    "    the gradient of the loss with respect to the output produced during the\n",
    "    forward pass. We can retrieve cached data from the context object, and must\n",
    "    compute and return the gradient of the loss with respect to the input to the\n",
    "    forward function.\n",
    "    \"\"\"\n",
    "    x, = ctx.saved_tensors\n",
    "    grad_x = grad_output.clone()\n",
    "    grad_x[x < 0] = 0\n",
    "    return grad_x\n",
    "\n",
    "\n",
    "class TanLU(torch.autograd.Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, x):\n",
    "    ctx.save_for_backward(x)\n",
    "    return torch.max(x, x.tanh())\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    x, = ctx.saved_tensors\n",
    "    derivative = (1.0 - x.tanh().pow(2))\n",
    "    derivative[x >= 0] = 1\n",
    "    return derivative * grad_output\n",
    "\n",
    "\n",
    "class SinLU(torch.autograd.Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, x):\n",
    "    ctx.save_for_backward(x)\n",
    "    return torch.max(x, x.sin())\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    x, = ctx.saved_tensors\n",
    "    return torch.max(x.abs() / x, x.cos()) * grad_output\n",
    "    \n",
    "\"\"\"@staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    x, = ctx.saved_tensors\n",
    "    derivative = x.cos()\n",
    "    derivative[x >= 0] = 1\n",
    "    return derivative * grad_output\"\"\"\n",
    "\n",
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and output\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "tic = time.time()\n",
    "learning_rate = 1e-6\n",
    "for t in range(15001):\n",
    "  # Forward pass: compute predicted y using operations on Tensors; we call our\n",
    "  # custom ReLU implementation using the MyReLU.apply function\n",
    "  #y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "  #y_pred = TanLU.apply(x.mm(w1)).mm(w2)\n",
    "  y_pred = SinLU.apply(x.mm(w1)).mm(w2)\n",
    "  #y_pred = F.relu(x.mm(w1)).mm(w2)\n",
    "\n",
    " \n",
    "  # Compute and print loss\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  if t % 500 == 0:\n",
    "    print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass.\n",
    "  loss.backward()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "    \n",
    "print(\"%f seconds\" % (time.time() - tic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28780567.84166371\n",
      "50 9181.32251046368\n",
      "100 239.82248553894152\n",
      "150 10.389517949663194\n",
      "200 0.5277910813482501\n",
      "250 0.029034486204176358\n",
      "300 0.0016846171303719604\n",
      "350 0.00010178335299635097\n",
      "400 6.353529894686354e-06\n",
      "450 4.0754736680296224e-07\n",
      "500 2.6766009550100157e-08\n",
      "550 1.7953261118913527e-09\n",
      "600 1.227776182430677e-10\n",
      "650 8.55065118004578e-12\n",
      "700 6.058354706981365e-13\n",
      "750 4.36399086264833e-14\n",
      "800 3.193750653299734e-15\n",
      "850 2.3728210792700794e-16\n",
      "900 1.7885525044032605e-17\n",
      "950 1.3662050020153237e-18\n",
      "1000 1.05868281838963e-19\n",
      "1050 9.106638679547816e-21\n",
      "1100 1.345539471399972e-21\n",
      "1150 3.8919537038164444e-22\n",
      "1200 1.7152115219569862e-22\n",
      "1250 9.607182952921477e-23\n",
      "1300 6.410461448269717e-23\n",
      "1350 4.741612016306246e-23\n",
      "1400 3.638092200533159e-23\n",
      "1450 2.956010319609922e-23\n",
      "1500 2.5171239965591414e-23\n",
      "1550 2.110716842759161e-23\n",
      "1600 1.8177211414913563e-23\n",
      "1650 1.5976113840993354e-23\n",
      "1700 1.410310557458918e-23\n",
      "1750 1.2742243473899044e-23\n",
      "1800 1.1582171912293511e-23\n",
      "1850 1.0817493293200936e-23\n",
      "1900 9.925682103094304e-24\n",
      "1950 9.058113848527839e-24\n",
      "2000 8.286197156095319e-24\n",
      "2050 7.654629174050017e-24\n",
      "2100 7.193481803625398e-24\n",
      "2150 6.828166318500322e-24\n",
      "2200 6.491300909487033e-24\n",
      "2250 6.082487744372258e-24\n",
      "2300 5.724708530678242e-24\n",
      "2350 5.541809145276387e-24\n",
      "2400 5.184880900372968e-24\n",
      "2450 4.861463108726062e-24\n",
      "2500 4.648784848646936e-24\n",
      "2550 4.333672491266392e-24\n",
      "2600 4.251151500243157e-24\n",
      "2650 4.00444689608053e-24\n",
      "2700 3.8571406576106e-24\n",
      "2750 3.666551338935888e-24\n",
      "2800 3.5272116422161915e-24\n",
      "2850 3.295526275611987e-24\n",
      "2900 3.2569866966846342e-24\n",
      "2950 3.1931110120276443e-24\n",
      "3000 3.0577708025628494e-24\n",
      "3050 2.951346839070818e-24\n",
      "3100 2.855784785894988e-24\n",
      "3150 2.858513043544917e-24\n",
      "3200 2.7194535410230415e-24\n",
      "3250 2.6423293209696834e-24\n",
      "3300 2.5918590996105665e-24\n",
      "3350 2.5261237810970535e-24\n",
      "3400 2.4996314696022104e-24\n",
      "3450 2.4154542962408203e-24\n",
      "3500 2.3364556537077214e-24\n",
      "3550 2.2602073228469563e-24\n",
      "3600 2.154824134472803e-24\n",
      "3650 2.123252602779913e-24\n",
      "3700 2.0656915146275233e-24\n",
      "3750 2.0019516566011585e-24\n",
      "3800 1.9326643417567135e-24\n",
      "3850 1.8478473794752158e-24\n",
      "3900 1.851691855673584e-24\n",
      "3950 1.8288500998993454e-24\n",
      "4000 1.7545998666384067e-24\n",
      "4050 1.746275905058387e-24\n",
      "4100 1.7173903936775294e-24\n",
      "4150 1.732138395851847e-24\n",
      "4200 1.7026819879788854e-24\n",
      "4250 1.714265377810295e-24\n",
      "4300 1.5879957167798042e-24\n",
      "4350 1.5858333180753572e-24\n",
      "4400 1.5571794286231499e-24\n",
      "4450 1.5443871245625067e-24\n",
      "4500 1.5170187782175908e-24\n",
      "4550 1.5119419401932644e-24\n",
      "4600 1.4698458371308563e-24\n",
      "4650 1.3988265341524716e-24\n",
      "4700 1.3466194216961756e-24\n",
      "4750 1.342516852011146e-24\n",
      "4800 1.2854744655428813e-24\n",
      "4850 1.2561625193910155e-24\n",
      "4900 1.273425875259058e-24\n",
      "4950 1.2608446818136666e-24\n",
      "5000 1.2133770935834458e-24\n",
      "5050 1.2246534645412731e-24\n",
      "5100 1.2020929181548785e-24\n",
      "5150 1.191023506018918e-24\n",
      "5200 1.1898380766019957e-24\n",
      "5250 1.2057216731580047e-24\n",
      "5300 1.1643404029482096e-24\n",
      "5350 1.1535318879924918e-24\n",
      "5400 1.180250894935336e-24\n",
      "5450 1.1400200045526653e-24\n",
      "5500 1.0854185717524613e-24\n",
      "5550 1.1117237003563414e-24\n",
      "5600 1.1150540688755403e-24\n",
      "5650 1.0653482173335066e-24\n",
      "5700 1.0567446698756855e-24\n",
      "5750 1.0211915807127584e-24\n",
      "5800 9.897581227412933e-25\n",
      "5850 9.699370995883449e-25\n",
      "5900 9.408453711725847e-25\n",
      "5950 9.072057248351663e-25\n",
      "6000 9.050093150625328e-25\n",
      "6050 9.062671999982264e-25\n",
      "6100 8.992916222603194e-25\n",
      "6150 9.290233870541109e-25\n",
      "6200 8.829669387200916e-25\n",
      "6250 8.814166837641815e-25\n",
      "6300 9.021477981790023e-25\n",
      "6350 8.900512457357887e-25\n",
      "6400 8.88430592202436e-25\n",
      "6450 8.97423922290207e-25\n",
      "6500 8.802392820866944e-25\n",
      "6550 9.013963110577804e-25\n",
      "6600 8.883175200308623e-25\n",
      "6650 8.542475339399826e-25\n",
      "6700 8.394939330000478e-25\n",
      "6750 8.473334326321983e-25\n",
      "6800 8.152019185688194e-25\n",
      "6850 8.22758719803711e-25\n",
      "6900 8.081747276176656e-25\n",
      "6950 7.803109472538393e-25\n",
      "7000 7.713440643851394e-25\n",
      "7050 7.312729369059536e-25\n",
      "7100 6.98042966381659e-25\n",
      "7150 6.804204448020049e-25\n",
      "7200 6.918861024621872e-25\n",
      "7250 6.894042788875551e-25\n",
      "7300 6.748300513529022e-25\n",
      "7350 6.8177541369350535e-25\n",
      "7400 6.987073545850375e-25\n",
      "7450 6.860029787589565e-25\n",
      "7500 7.0009263042096565e-25\n",
      "7550 6.69968447085243e-25\n",
      "7600 6.602953747810387e-25\n",
      "7650 6.781186373026878e-25\n",
      "7700 6.555952598844135e-25\n",
      "7750 6.639803586088941e-25\n",
      "7800 6.70862913316597e-25\n",
      "7850 6.676333889538747e-25\n",
      "7900 6.622883355495453e-25\n",
      "7950 6.35269519007999e-25\n",
      "8000 6.442396582992341e-25\n",
      "8050 6.10161425472629e-25\n",
      "8100 6.217519402863818e-25\n",
      "8150 6.072701581541653e-25\n",
      "8200 5.965451167939118e-25\n",
      "8250 5.610733642717588e-25\n",
      "8300 5.544591563406293e-25\n",
      "8350 5.591487174631507e-25\n",
      "8400 5.593289616716384e-25\n",
      "8450 5.600741969293529e-25\n",
      "8500 5.446301005236962e-25\n",
      "8550 5.440199651469424e-25\n",
      "8600 5.479917948712196e-25\n",
      "8650 5.4186962422043175e-25\n",
      "8700 5.38339340417442e-25\n",
      "8750 5.28031923086144e-25\n",
      "8800 5.368688277234585e-25\n",
      "8850 5.394185394258506e-25\n",
      "8900 5.219010294472481e-25\n",
      "8950 5.258417559445913e-25\n",
      "9000 5.296414655504799e-25\n",
      "9050 5.297667424665004e-25\n",
      "9100 5.393320532847487e-25\n",
      "9150 5.3662951018199175e-25\n",
      "9200 5.610273224029492e-25\n",
      "9250 5.276062990744665e-25\n",
      "9300 5.003905667887212e-25\n",
      "9350 5.071549436304011e-25\n",
      "9400 5.141005033186382e-25\n",
      "9450 4.976521694907839e-25\n",
      "9500 5.09077894100979e-25\n",
      "9550 4.91961553602474e-25\n",
      "9600 4.940696737751501e-25\n",
      "9650 5.156609985885074e-25\n",
      "9700 4.918925649381585e-25\n",
      "9750 5.025675150272296e-25\n",
      "9800 5.029443727533597e-25\n",
      "9850 4.862335170157153e-25\n",
      "9900 4.812976533318288e-25\n",
      "9950 4.677897715651666e-25\n",
      "10000 4.533360697676393e-25\n",
      "10050 4.396520403056664e-25\n",
      "10100 4.47768623888176e-25\n",
      "10150 4.389752022110333e-25\n",
      "10200 4.565875638120811e-25\n",
      "10250 4.471351902239213e-25\n",
      "10300 4.37538647451387e-25\n",
      "10350 4.521310574438851e-25\n",
      "10400 4.528914547776797e-25\n",
      "10450 4.921045323063553e-25\n",
      "10500 4.789724283367181e-25\n",
      "10550 4.811806162542971e-25\n",
      "10600 4.954897212658632e-25\n",
      "10650 4.73158182563602e-25\n",
      "10700 4.905408503928001e-25\n",
      "10750 4.736274555325569e-25\n",
      "10800 4.578760944700505e-25\n",
      "10850 4.78009670883476e-25\n",
      "10900 4.581901139289572e-25\n",
      "10950 4.586255805560787e-25\n",
      "11000 4.327459907332006e-25\n",
      "11050 4.215558844405501e-25\n",
      "11100 4.014556996680927e-25\n",
      "11150 3.853522338338811e-25\n",
      "11200 3.883080385810418e-25\n",
      "11250 3.7847762241057593e-25\n",
      "11300 4.075684355040559e-25\n",
      "11350 4.023721712349443e-25\n",
      "11400 3.9989105974582574e-25\n",
      "11450 3.880596202141047e-25\n",
      "11500 3.992910404124013e-25\n",
      "11550 3.866655348639043e-25\n",
      "11600 3.9855266207015125e-25\n",
      "11650 4.035312535901803e-25\n",
      "11700 4.0822153431836984e-25\n",
      "11750 4.0733158838001224e-25\n",
      "11800 3.9916064342667e-25\n",
      "11850 4.0399462666835286e-25\n",
      "11900 3.8519809989098364e-25\n",
      "11950 4.040827308975531e-25\n",
      "12000 3.9468110460949067e-25\n",
      "12050 3.9585375857475685e-25\n",
      "12100 3.970288351311889e-25\n",
      "12150 3.9740391826935706e-25\n",
      "12200 3.7244350815972182e-25\n",
      "12250 3.850749747773696e-25\n",
      "12300 3.8589047289465073e-25\n",
      "12350 3.695081861259002e-25\n",
      "12400 3.5031583643653363e-25\n",
      "12450 3.6045042850555276e-25\n",
      "12500 3.628986464202517e-25\n",
      "12550 3.6027688362030244e-25\n",
      "12600 3.4456914275251343e-25\n",
      "12650 3.476352846731802e-25\n",
      "12700 3.4412844722559287e-25\n",
      "12750 3.402761789072625e-25\n",
      "12800 3.668866534745793e-25\n",
      "12850 3.601285885553286e-25\n",
      "12900 3.5525799502244335e-25\n",
      "12950 3.596507448249722e-25\n",
      "13000 3.4100914538486417e-25\n",
      "13050 3.490901544172214e-25\n",
      "13100 3.587063261353412e-25\n",
      "13150 3.648581283257332e-25\n",
      "13200 3.3195030007451576e-25\n",
      "13250 3.3802475746000896e-25\n",
      "13300 3.4617133835308263e-25\n",
      "13350 3.379366913679832e-25\n",
      "13400 3.4246572242893757e-25\n",
      "13450 3.3424797310478964e-25\n",
      "13500 3.252787498099102e-25\n",
      "13550 3.2004118519189373e-25\n",
      "13600 3.175051173670437e-25\n",
      "13650 3.1847881997645627e-25\n",
      "13700 3.203859444276706e-25\n",
      "13750 3.145541947559955e-25\n",
      "13800 3.045415436508396e-25\n",
      "13850 2.9996455437488966e-25\n",
      "13900 3.088775360977912e-25\n",
      "13950 3.036464099117271e-25\n",
      "14000 3.066004101678542e-25\n",
      "14050 3.0041249822341536e-25\n",
      "14100 3.0614752130912647e-25\n",
      "14150 2.90513243077025e-25\n",
      "14200 2.8985627381458905e-25\n",
      "14250 2.795706401720877e-25\n",
      "14300 2.91331293737595e-25\n",
      "14350 2.8937123703681694e-25\n",
      "14400 2.691686191404472e-25\n",
      "14450 2.764813817717068e-25\n",
      "14500 2.7196826630692336e-25\n",
      "14550 2.812605487619783e-25\n",
      "14600 2.736368819477565e-25\n",
      "14650 2.887383099282488e-25\n",
      "14700 3.073783318466625e-25\n",
      "14750 2.9464414452765053e-25\n",
      "14800 2.8418762077395656e-25\n",
      "14850 2.870247249306239e-25\n",
      "14900 2.903056817430214e-25\n",
      "14950 2.852023496874892e-25\n",
      "15000 3.0142402928224326e-25\n",
      "13.922092 seconds\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import time\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = cp.random.randn(N, D_in)\n",
    "y = cp.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = cp.random.randn(D_in, H)\n",
    "w2 = cp.random.randn(H, D_out)\n",
    "\n",
    "\n",
    "activation_function = \"relu\"\n",
    "#activation_function = \"sinlu\"\n",
    "\n",
    "tic = time.time()\n",
    "learning_rate = 1e-6\n",
    "for t in range(15001):\n",
    "  if activation_function == \"relu\":\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = cp.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "  \n",
    "  elif activation_function == \"sinlu\":\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = cp.maximum(h, cp.sin(h))\n",
    "    y_pred = h_relu.dot(w2)\n",
    "  \n",
    "  # Compute and print loss\n",
    "  loss = cp.square(y_pred - y).sum()\n",
    "  if t % 50 == 0:\n",
    "    print(t, loss)\n",
    "  \n",
    "  if activation_function == \"relu\":\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "  elif activation_function == \"sinlu\":\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = cp.maximum(cp.abs(h_relu) / h_relu, cp.cos(h_relu)) * grad_h_relu\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "  \n",
    " \n",
    "  # Update weights\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2\n",
    "    \n",
    "print(\"%f seconds\" % (time.time() - tic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
